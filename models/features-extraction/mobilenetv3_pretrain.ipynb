{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor, Normalize, Resize, Compose\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import random_split\n",
    "import time\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "class RandomOneTransform:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Randomly choose one transformation to apply\n",
    "        transform = random.choice(self.transforms)\n",
    "        return transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, filename=\"model_state_dict.pth\"):\n",
    "  torch.save(model, filename)\n",
    "  torch.save(model.state_dict(), f\"s_{filename}.pth\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate(device, model, val_loader):\n",
    "    model.eval()\n",
    "    num_classes = len(val_loader.dataset.classes)  # Assuming dataset classes are accessible like this\n",
    "    confusion_matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        class_correct = {}\n",
    "        class_total = {}\n",
    "        false_positives = {}\n",
    "        false_negatives = {}\n",
    "\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            for label, prediction in zip(labels, predicted):\n",
    "                confusion_matrix[label.item(), prediction.item()] += 1  # Update the confusion matrix\n",
    "                if label == prediction:\n",
    "                    class_correct[label.item()] = class_correct.get(label.item(), 0) + 1\n",
    "                else:\n",
    "                    false_negatives[label.item()] = false_negatives.get(label.item(), 0) + 1\n",
    "                    false_positives[prediction.item()] = false_positives.get(prediction.item(), 0) + 1\n",
    "\n",
    "                class_total[label.item()] = class_total.get(label.item(), 0) + 1\n",
    "\n",
    "        # Calculate precision and recall\n",
    "        precision_list = []\n",
    "        recall_list = []\n",
    "\n",
    "        for class_id in class_total.keys():\n",
    "            tp = class_correct.get(class_id, 0)\n",
    "            fp = false_positives.get(class_id, 0)\n",
    "            fn = false_negatives.get(class_id, 0)\n",
    "\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "            precision_list.append(precision)\n",
    "            recall_list.append(recall)\n",
    "\n",
    "        # Calculate overall precision, recall\n",
    "        overall_precision = sum(precision_list) / len(precision_list) if len(precision_list) > 0 else 0\n",
    "        overall_recall = sum(recall_list) / len(recall_list) if len(recall_list) > 0 else 0\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        print(f'Accuracy: {accuracy:.2f}%')\n",
    "        print(f'Precision: {overall_precision:.2f}')\n",
    "        print(f'Recall: {overall_recall:.2f}')\n",
    "\n",
    "        return accuracy, overall_precision, overall_recall, confusion_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device, model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.to(device).float()\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Start epoch {epoch+1}/{num_epochs}')\n",
    "        running_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracy = train_correct / train_total\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {train_accuracy:.2f}')\n",
    "\n",
    "    end = time.time()\n",
    "    computation_time = end - start \n",
    "    print(f'Training completed in {(end - start):.2f} seconds')\n",
    "    print(f'Training accuracy: {train_accuracy:.2f}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    return train_accuracy, train_losses, computation_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: mps , pretrained: pretrained\n",
      "Number of parameters in the model: 4212280\n",
      "Number of trainable parameters in the model: 10248\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "MobileNetV3                                   [1, 8]                    --\n",
       "├─Conv2d: 1-1                                 [1, 16, 112, 112]         (432)\n",
       "├─BatchNormAct2d: 1-2                         [1, 16, 112, 112]         32\n",
       "│    └─Identity: 2-1                          [1, 16, 112, 112]         --\n",
       "│    └─Hardswish: 2-2                         [1, 16, 112, 112]         --\n",
       "├─Sequential: 1-3                             [1, 960, 7, 7]            --\n",
       "│    └─Sequential: 2-3                        [1, 16, 112, 112]         --\n",
       "│    │    └─DepthwiseSeparableConv: 3-1       [1, 16, 112, 112]         (464)\n",
       "│    └─Sequential: 2-4                        [1, 24, 56, 56]           --\n",
       "│    │    └─InvertedResidual: 3-2             [1, 24, 56, 56]           (3,440)\n",
       "│    │    └─InvertedResidual: 3-3             [1, 24, 56, 56]           (4,440)\n",
       "│    └─Sequential: 2-5                        [1, 40, 28, 28]           --\n",
       "│    │    └─InvertedResidual: 3-4             [1, 40, 28, 28]           (10,328)\n",
       "│    │    └─InvertedResidual: 3-5             [1, 40, 28, 28]           (20,992)\n",
       "│    │    └─InvertedResidual: 3-6             [1, 40, 28, 28]           (20,992)\n",
       "│    └─Sequential: 2-6                        [1, 80, 14, 14]           --\n",
       "│    │    └─InvertedResidual: 3-7             [1, 80, 14, 14]           (32,080)\n",
       "│    │    └─InvertedResidual: 3-8             [1, 80, 14, 14]           (34,760)\n",
       "│    │    └─InvertedResidual: 3-9             [1, 80, 14, 14]           (31,992)\n",
       "│    │    └─InvertedResidual: 3-10            [1, 80, 14, 14]           (31,992)\n",
       "│    └─Sequential: 2-7                        [1, 112, 14, 14]          --\n",
       "│    │    └─InvertedResidual: 3-11            [1, 112, 14, 14]          (214,424)\n",
       "│    │    └─InvertedResidual: 3-12            [1, 112, 14, 14]          (386,120)\n",
       "│    └─Sequential: 2-8                        [1, 160, 7, 7]            --\n",
       "│    │    └─InvertedResidual: 3-13            [1, 160, 7, 7]            (429,224)\n",
       "│    │    └─InvertedResidual: 3-14            [1, 160, 7, 7]            (797,360)\n",
       "│    │    └─InvertedResidual: 3-15            [1, 160, 7, 7]            (797,360)\n",
       "│    └─Sequential: 2-9                        [1, 960, 7, 7]            --\n",
       "│    │    └─ConvBnAct: 3-16                   [1, 960, 7, 7]            (155,520)\n",
       "├─SelectAdaptivePool2d: 1-4                   [1, 960, 1, 1]            --\n",
       "│    └─AdaptiveAvgPool2d: 2-10                [1, 960, 1, 1]            --\n",
       "│    └─Identity: 2-11                         [1, 960, 1, 1]            --\n",
       "├─Conv2d: 1-5                                 [1, 1280, 1, 1]           (1,230,080)\n",
       "├─Hardswish: 1-6                              [1, 1280, 1, 1]           --\n",
       "├─Flatten: 1-7                                [1, 1280]                 --\n",
       "├─Sequential: 1-8                             [1, 8]                    --\n",
       "│    └─Linear: 2-12                           [1, 8]                    10,248\n",
       "===============================================================================================\n",
       "Total params: 4,212,280\n",
       "Trainable params: 10,248\n",
       "Non-trainable params: 4,202,032\n",
       "Total mult-adds (Units.MEGABYTES): 215.33\n",
       "===============================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 35.25\n",
       "Params size (MB): 16.75\n",
       "Estimated Total Size (MB): 52.60\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the device for training\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# Load the pre-trained MobileNetV3 model\n",
    " \n",
    "model_name = 'mobilenetv3_large_100'\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "is_pretrained = True\n",
    "is_pretrained_str = \"pretrained\" if is_pretrained else \"not_pretrained\"\n",
    "file_name = f\"{model_name}-lr_{learning_rate}-batch_{batch_size}-pretrained_{is_pretrained_str}\"\n",
    "\n",
    "model = timm.create_model(model_name, pretrained=True).to(device)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "num_classes = 8\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(model.classifier.in_features, num_classes)\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "# Define path to PlantVillage dataset\n",
    "dataset_path_training = '../dataset-tomatoes/train'\n",
    "\n",
    "# Load the PlantVillage dataset with appropriate transforms\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    RandomOneTransform([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.5),\n",
    "        transforms.ColorJitter(contrast=0.5),\n",
    "        transforms.ColorJitter(saturation=0.5),\n",
    "        transforms.RandomRotation(45),\n",
    "        transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0), ratio=(0.75, 1.33))\n",
    "    ]),\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "data_transforms_validation_test = transforms.Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "# Ebable training mode\n",
    "\n",
    "# Fine-tune the model\n",
    "num_epochs = 25\n",
    "# Freeze all the parameters in the model\n",
    "\n",
    "#train(device, model, train_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "print(\"Training on:\", device, \", pretrained:\", is_pretrained_str)\n",
    "print(f\"Number of parameters in the model: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"Number of trainable parameters in the model: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "summary(model=model, input_size=(1, 3, 224, 224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch 1/25\n",
      "Epoch 1/25, Loss: 0.7265, Accuracy: 0.79\n",
      "Start epoch 2/25\n",
      "Epoch 2/25, Loss: 0.3640, Accuracy: 0.90\n",
      "Start epoch 3/25\n",
      "Epoch 3/25, Loss: 0.3013, Accuracy: 0.91\n",
      "Start epoch 4/25\n",
      "Epoch 4/25, Loss: 0.2601, Accuracy: 0.92\n",
      "Start epoch 5/25\n",
      "Epoch 5/25, Loss: 0.2396, Accuracy: 0.93\n",
      "Start epoch 6/25\n",
      "Epoch 6/25, Loss: 0.2198, Accuracy: 0.93\n",
      "Start epoch 7/25\n",
      "Epoch 7/25, Loss: 0.2073, Accuracy: 0.94\n",
      "Start epoch 8/25\n",
      "Epoch 8/25, Loss: 0.1967, Accuracy: 0.94\n",
      "Start epoch 9/25\n",
      "Epoch 9/25, Loss: 0.1891, Accuracy: 0.94\n",
      "Start epoch 10/25\n",
      "Epoch 10/25, Loss: 0.1813, Accuracy: 0.94\n",
      "Start epoch 11/25\n",
      "Epoch 11/25, Loss: 0.1732, Accuracy: 0.95\n",
      "Start epoch 12/25\n",
      "Epoch 12/25, Loss: 0.1725, Accuracy: 0.95\n",
      "Start epoch 13/25\n",
      "Epoch 13/25, Loss: 0.1695, Accuracy: 0.95\n",
      "Start epoch 14/25\n",
      "Epoch 14/25, Loss: 0.1708, Accuracy: 0.95\n",
      "Start epoch 15/25\n",
      "Epoch 15/25, Loss: 0.1593, Accuracy: 0.95\n",
      "Start epoch 16/25\n",
      "Epoch 16/25, Loss: 0.1579, Accuracy: 0.95\n",
      "Start epoch 17/25\n",
      "Epoch 17/25, Loss: 0.1514, Accuracy: 0.95\n",
      "Start epoch 18/25\n",
      "Epoch 18/25, Loss: 0.1486, Accuracy: 0.95\n",
      "Start epoch 19/25\n",
      "Epoch 19/25, Loss: 0.1433, Accuracy: 0.96\n",
      "Start epoch 20/25\n",
      "Epoch 20/25, Loss: 0.1428, Accuracy: 0.95\n",
      "Start epoch 21/25\n",
      "Epoch 21/25, Loss: 0.1447, Accuracy: 0.95\n",
      "Start epoch 22/25\n",
      "Epoch 22/25, Loss: 0.1374, Accuracy: 0.96\n",
      "Start epoch 23/25\n",
      "Epoch 23/25, Loss: 0.1338, Accuracy: 0.96\n",
      "Start epoch 24/25\n",
      "Epoch 24/25, Loss: 0.1483, Accuracy: 0.96\n",
      "Start epoch 25/25\n",
      "Epoch 25/25, Loss: 0.1376, Accuracy: 0.96\n",
      "Training completed in 1265.49 seconds\n",
      "Training accuracy: 0.96\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ImageFolder(root=dataset_path_training, transform=data_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Run training and validation\n",
    "train_accuracy, train_losses, computation_time = train(device, model, train_loader, criterion, optimizer, num_epochs=25)\n",
    "\n",
    "save_model(model, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.54%\n",
      "Precision: 0.94\n",
      "Recall: 0.92\n",
      "Validation Accuracy: 94.543429844098%\n",
      "Validation Precision: 0.9372580274091409\n",
      "Validation Recall: 0.9205599685964514\n",
      "Confusion Matrix:\n",
      "[[223   2   3   1   6   0   0   0]\n",
      " [  4 118   5   0   3   1   0   0]\n",
      " [  1  10 193   1   3   1   0   1]\n",
      " [  1   0   4 110   5   0   0   0]\n",
      " [ 13   6   1   3 180   4   0   3]\n",
      " [  2   0   1   0   0 620   0   1]\n",
      " [  2   2   0   2   1   3  55   1]\n",
      " [  0   0   0   0   0   1   0 199]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "validation_dataset = ImageFolder(root='../dataset-tomatoes/validation', transform=data_transforms_validation_test)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "try:\n",
    "    validation_accuracy, validation_precision, validation_recall, confusion_matrix = validate(device, model, validation_loader)\n",
    "    print(f\"Validation Accuracy: {validation_accuracy}%\")\n",
    "    print(f\"Validation Precision: {validation_precision}\")\n",
    "    print(f\"Validation Recall: {validation_recall}\")\n",
    "    print(f\"Confusion Matrix:\\n{confusion_matrix}\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"Runtime error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.54%\n",
      "Precision: 0.94\n",
      "Recall: 0.92\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_dataset = ImageFolder(root='../dataset-tomatoes/test', transform=data_transforms_validation_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_accuracy, test_precision, test_recall, test_matrix = validate(device, model, validation_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          model     params  tr_params  learning_rate  batch  \\\n",
      "0          vit_tiny_patch16_224    5525960     149576          0.001     64   \n",
      "1                         vgg16  134293320   16814088          0.001     64   \n",
      "2  swin_tiny_patch4_window7_224   27525506    2366216          0.001     64   \n",
      "3            shufflenet_v2_x1_0    1261804     486080          0.001     64   \n",
      "4                      resnet50   23524424    1069064          0.001     64   \n",
      "5               mobilevit-small    4942760     108808          0.001     64   \n",
      "\n",
      "   accuracy_(Tr)  accuracy_(Va)  precision_(Va)  recall_(Va)  accuracy_(Te)  \\\n",
      "0       0.921827      92.873051        0.910795     0.892480      92.873051   \n",
      "1       0.979658      95.322940        0.941256     0.925700      95.322940   \n",
      "2       0.978756      96.380846        0.954280     0.947698      96.380846   \n",
      "3       0.990489      97.438753        0.964152     0.964447      97.438753   \n",
      "4       0.958206      92.873051        0.918170     0.898502      92.873051   \n",
      "5       0.973757      95.322940        0.940809     0.928922      95.322940   \n",
      "\n",
      "   precision_(Te)  recall_(Te)     time_(s)  \n",
      "0        0.910795     0.892480  1558.915665  \n",
      "1        0.941256     0.925700  4195.398219  \n",
      "2        0.954280     0.947698  4775.044521  \n",
      "3        0.964152     0.964447  1618.468904  \n",
      "4        0.918170     0.898502  2411.250841  \n",
      "5        0.940809     0.928922  2514.299880  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "csv_path = '../results.csv'\n",
    "# append to a csv model learning_rate\tbatch\taccuracy (Tr)\tprecision (Tr)\taccuracy (Va)\tprecision (Va)\taccuracy (Te)\tprecision (Te)\ttime (s)\n",
    "# check if the file exists\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "row = pd.DataFrame({\n",
    "    'model': [model_name],\n",
    "    'params': [total_params],\n",
    "    'tr_params': [total_trainable_params],\n",
    "    'learning_rate': [learning_rate],\n",
    "    'batch': [batch_size],\n",
    "    'accuracy_(Tr)': [train_accuracy],\n",
    "    'accuracy_(Va)': [validation_accuracy],\n",
    "    'precision_(Va)': [validation_precision],\n",
    "    'accuracy_(Te)': [test_accuracy],\n",
    "    'precision_(Te)': [test_precision],\n",
    "    'time_(s)': [computation_time]\n",
    "}, index=[0])\n",
    "print(df)\n",
    "\n",
    "# add the row to the dataframe csv file\n",
    "df = pd.concat([row], ignore_index=False)\n",
    "\n",
    "\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
