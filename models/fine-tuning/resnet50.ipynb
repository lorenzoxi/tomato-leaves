{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import timm\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor, Normalize, Resize, Compose\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import random_split\n",
    "import time\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "class RandomOneTransform:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Randomly choose one transformation to apply\n",
    "        transform = random.choice(self.transforms)\n",
    "        return transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, filename=\"model_state_dict.pth\"):\n",
    "  torch.save(model, f\"{filename}.pth\")\n",
    "  torch.save(model.state_dict(), f\"s_{filename}.pth\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate(device, model, val_loader):\n",
    "    model.to(device).float()\n",
    "    model.eval()\n",
    "    num_classes = len(val_loader.dataset.classes)  \n",
    "    confusion_matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        class_correct = {}\n",
    "        class_total = {}\n",
    "        false_positives = {}\n",
    "        false_negatives = {}\n",
    "\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            for label, prediction in zip(labels, predicted):\n",
    "                confusion_matrix[label.item(), prediction.item()] += 1  # Update the confusion matrix\n",
    "                if label == prediction:\n",
    "                    class_correct[label.item()] = class_correct.get(label.item(), 0) + 1\n",
    "                else:\n",
    "                    false_negatives[label.item()] = false_negatives.get(label.item(), 0) + 1\n",
    "                    false_positives[prediction.item()] = false_positives.get(prediction.item(), 0) + 1\n",
    "\n",
    "                class_total[label.item()] = class_total.get(label.item(), 0) + 1\n",
    "\n",
    "        # Calculate precision and recall\n",
    "        precision_list = []\n",
    "        recall_list = []\n",
    "\n",
    "        for class_id in class_total.keys():\n",
    "            tp = class_correct.get(class_id, 0)\n",
    "            fp = false_positives.get(class_id, 0)\n",
    "            fn = false_negatives.get(class_id, 0)\n",
    "\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "            precision_list.append(precision)\n",
    "            recall_list.append(recall)\n",
    "\n",
    "        # Calculate overall precision, recall\n",
    "        overall_precision = sum(precision_list) / len(precision_list) if len(precision_list) > 0 else 0\n",
    "        overall_recall = sum(recall_list) / len(recall_list) if len(recall_list) > 0 else 0\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        print(f'Accuracy: {accuracy:.2f}%')\n",
    "        print(f'Precision: {overall_precision:.2f}')\n",
    "        print(f'Recall: {overall_recall:.2f}')\n",
    "\n",
    "        return accuracy, overall_precision, overall_recall, confusion_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device, model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.to(device).float()\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Start epoch {epoch+1}/{num_epochs}')\n",
    "        running_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracy = train_correct / train_total\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {train_accuracy:.2f}')\n",
    "\n",
    "    end = time.time()\n",
    "    computation_time = end - start \n",
    "    print(f'Training completed in {(end - start):.2f} seconds')\n",
    "    print(f'Training accuracy: {train_accuracy:.2f}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    return train_accuracy, train_losses, computation_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: mps , pretrained: pretrained\n",
      "Number of parameters in the model: 23524424\n",
      "Number of trainable parameters in the model: 1069064\n",
      "conv1.weight False\n",
      "bn1.weight False\n",
      "bn1.bias False\n",
      "layer1.0.conv1.weight False\n",
      "layer1.0.bn1.weight False\n",
      "layer1.0.bn1.bias False\n",
      "layer1.0.conv2.weight False\n",
      "layer1.0.bn2.weight False\n",
      "layer1.0.bn2.bias False\n",
      "layer1.0.conv3.weight False\n",
      "layer1.0.bn3.weight False\n",
      "layer1.0.bn3.bias False\n",
      "layer1.0.downsample.0.weight False\n",
      "layer1.0.downsample.1.weight False\n",
      "layer1.0.downsample.1.bias False\n",
      "layer1.1.conv1.weight False\n",
      "layer1.1.bn1.weight False\n",
      "layer1.1.bn1.bias False\n",
      "layer1.1.conv2.weight False\n",
      "layer1.1.bn2.weight False\n",
      "layer1.1.bn2.bias False\n",
      "layer1.1.conv3.weight False\n",
      "layer1.1.bn3.weight False\n",
      "layer1.1.bn3.bias False\n",
      "layer1.2.conv1.weight False\n",
      "layer1.2.bn1.weight False\n",
      "layer1.2.bn1.bias False\n",
      "layer1.2.conv2.weight False\n",
      "layer1.2.bn2.weight False\n",
      "layer1.2.bn2.bias False\n",
      "layer1.2.conv3.weight False\n",
      "layer1.2.bn3.weight False\n",
      "layer1.2.bn3.bias False\n",
      "layer2.0.conv1.weight False\n",
      "layer2.0.bn1.weight False\n",
      "layer2.0.bn1.bias False\n",
      "layer2.0.conv2.weight False\n",
      "layer2.0.bn2.weight False\n",
      "layer2.0.bn2.bias False\n",
      "layer2.0.conv3.weight False\n",
      "layer2.0.bn3.weight False\n",
      "layer2.0.bn3.bias False\n",
      "layer2.0.downsample.0.weight False\n",
      "layer2.0.downsample.1.weight False\n",
      "layer2.0.downsample.1.bias False\n",
      "layer2.1.conv1.weight False\n",
      "layer2.1.bn1.weight False\n",
      "layer2.1.bn1.bias False\n",
      "layer2.1.conv2.weight False\n",
      "layer2.1.bn2.weight False\n",
      "layer2.1.bn2.bias False\n",
      "layer2.1.conv3.weight False\n",
      "layer2.1.bn3.weight False\n",
      "layer2.1.bn3.bias False\n",
      "layer2.2.conv1.weight False\n",
      "layer2.2.bn1.weight False\n",
      "layer2.2.bn1.bias False\n",
      "layer2.2.conv2.weight False\n",
      "layer2.2.bn2.weight False\n",
      "layer2.2.bn2.bias False\n",
      "layer2.2.conv3.weight False\n",
      "layer2.2.bn3.weight False\n",
      "layer2.2.bn3.bias False\n",
      "layer2.3.conv1.weight False\n",
      "layer2.3.bn1.weight False\n",
      "layer2.3.bn1.bias False\n",
      "layer2.3.conv2.weight False\n",
      "layer2.3.bn2.weight False\n",
      "layer2.3.bn2.bias False\n",
      "layer2.3.conv3.weight False\n",
      "layer2.3.bn3.weight False\n",
      "layer2.3.bn3.bias False\n",
      "layer3.0.conv1.weight False\n",
      "layer3.0.bn1.weight False\n",
      "layer3.0.bn1.bias False\n",
      "layer3.0.conv2.weight False\n",
      "layer3.0.bn2.weight False\n",
      "layer3.0.bn2.bias False\n",
      "layer3.0.conv3.weight False\n",
      "layer3.0.bn3.weight False\n",
      "layer3.0.bn3.bias False\n",
      "layer3.0.downsample.0.weight False\n",
      "layer3.0.downsample.1.weight False\n",
      "layer3.0.downsample.1.bias False\n",
      "layer3.1.conv1.weight False\n",
      "layer3.1.bn1.weight False\n",
      "layer3.1.bn1.bias False\n",
      "layer3.1.conv2.weight False\n",
      "layer3.1.bn2.weight False\n",
      "layer3.1.bn2.bias False\n",
      "layer3.1.conv3.weight False\n",
      "layer3.1.bn3.weight False\n",
      "layer3.1.bn3.bias False\n",
      "layer3.2.conv1.weight False\n",
      "layer3.2.bn1.weight False\n",
      "layer3.2.bn1.bias False\n",
      "layer3.2.conv2.weight False\n",
      "layer3.2.bn2.weight False\n",
      "layer3.2.bn2.bias False\n",
      "layer3.2.conv3.weight False\n",
      "layer3.2.bn3.weight False\n",
      "layer3.2.bn3.bias False\n",
      "layer3.3.conv1.weight False\n",
      "layer3.3.bn1.weight False\n",
      "layer3.3.bn1.bias False\n",
      "layer3.3.conv2.weight False\n",
      "layer3.3.bn2.weight False\n",
      "layer3.3.bn2.bias False\n",
      "layer3.3.conv3.weight False\n",
      "layer3.3.bn3.weight False\n",
      "layer3.3.bn3.bias False\n",
      "layer3.4.conv1.weight False\n",
      "layer3.4.bn1.weight False\n",
      "layer3.4.bn1.bias False\n",
      "layer3.4.conv2.weight False\n",
      "layer3.4.bn2.weight False\n",
      "layer3.4.bn2.bias False\n",
      "layer3.4.conv3.weight False\n",
      "layer3.4.bn3.weight False\n",
      "layer3.4.bn3.bias False\n",
      "layer3.5.conv1.weight False\n",
      "layer3.5.bn1.weight False\n",
      "layer3.5.bn1.bias False\n",
      "layer3.5.conv2.weight False\n",
      "layer3.5.bn2.weight False\n",
      "layer3.5.bn2.bias False\n",
      "layer3.5.conv3.weight False\n",
      "layer3.5.bn3.weight False\n",
      "layer3.5.bn3.bias False\n",
      "layer4.0.conv1.weight False\n",
      "layer4.0.bn1.weight False\n",
      "layer4.0.bn1.bias False\n",
      "layer4.0.conv2.weight False\n",
      "layer4.0.bn2.weight False\n",
      "layer4.0.bn2.bias False\n",
      "layer4.0.conv3.weight False\n",
      "layer4.0.bn3.weight False\n",
      "layer4.0.bn3.bias False\n",
      "layer4.0.downsample.0.weight False\n",
      "layer4.0.downsample.1.weight False\n",
      "layer4.0.downsample.1.bias False\n",
      "layer4.1.conv1.weight False\n",
      "layer4.1.bn1.weight False\n",
      "layer4.1.bn1.bias False\n",
      "layer4.1.conv2.weight False\n",
      "layer4.1.bn2.weight False\n",
      "layer4.1.bn2.bias False\n",
      "layer4.1.conv3.weight False\n",
      "layer4.1.bn3.weight False\n",
      "layer4.1.bn3.bias False\n",
      "layer4.2.conv1.weight False\n",
      "layer4.2.bn1.weight False\n",
      "layer4.2.bn1.bias False\n",
      "layer4.2.conv2.weight False\n",
      "layer4.2.bn2.weight False\n",
      "layer4.2.bn2.bias False\n",
      "layer4.2.conv3.weight True\n",
      "layer4.2.bn3.weight True\n",
      "layer4.2.bn3.bias True\n",
      "fc.weight True\n",
      "fc.bias True\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "model_name = 'resnet50'\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "is_pretrained = True\n",
    "is_pretrained_str = \"pretrained\" if is_pretrained else \"not_pretrained\"\n",
    "file_name = f\"{model_name}-lr_{learning_rate}-batch_{batch_size}-pretrained_{is_pretrained_str}\"\n",
    "num_classes = 8\n",
    "\n",
    "\n",
    "model = timm.create_model(model_name, pretrained=True).to(device)    \n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_classes = 8\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "dataset_path_training = '../dataset-tomatoes/train'\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    RandomOneTransform([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.5),\n",
    "        transforms.ColorJitter(contrast=0.5),\n",
    "        transforms.ColorJitter(saturation=0.5),\n",
    "        transforms.RandomRotation(45),\n",
    "        transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0), ratio=(0.75, 1.33))\n",
    "    ]),\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "data_transforms_validation_test = transforms.Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "num_epochs = 25\n",
    "\n",
    "# Unfreeze the layer4.2.conv3 layer\n",
    "for name, param in model.named_parameters():\n",
    "    if name ==\"layer4.2.conv3.weight\" or name == \"layer4.2.bn3.weight\" or name == \"layer4.2.bn3.bias\":\n",
    "        param.requires_grad = True\n",
    "\n",
    "train(device, model, train_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "print(\"Training on:\", device, \", pretrained:\", is_pretrained_str)\n",
    "print(f\"Number of parameters in the model: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"Number of trainable parameters in the model: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch 1/25\n",
      "Epoch 1/25, Loss: 1.0629, Accuracy: 0.66\n",
      "Start epoch 2/25\n",
      "Epoch 2/25, Loss: 0.5448, Accuracy: 0.83\n",
      "Start epoch 3/25\n",
      "Epoch 3/25, Loss: 0.4378, Accuracy: 0.86\n",
      "Start epoch 4/25\n",
      "Epoch 4/25, Loss: 0.3812, Accuracy: 0.88\n",
      "Start epoch 5/25\n",
      "Epoch 5/25, Loss: 0.3535, Accuracy: 0.89\n",
      "Start epoch 6/25\n",
      "Epoch 6/25, Loss: 0.3219, Accuracy: 0.90\n",
      "Start epoch 7/25\n",
      "Epoch 7/25, Loss: 0.2915, Accuracy: 0.91\n",
      "Start epoch 8/25\n",
      "Epoch 8/25, Loss: 0.2669, Accuracy: 0.92\n",
      "Start epoch 9/25\n",
      "Epoch 9/25, Loss: 0.2541, Accuracy: 0.92\n",
      "Start epoch 10/25\n",
      "Epoch 10/25, Loss: 0.2368, Accuracy: 0.92\n",
      "Start epoch 11/25\n",
      "Epoch 11/25, Loss: 0.2279, Accuracy: 0.93\n",
      "Start epoch 12/25\n",
      "Epoch 12/25, Loss: 0.2176, Accuracy: 0.93\n",
      "Start epoch 13/25\n",
      "Epoch 13/25, Loss: 0.2066, Accuracy: 0.93\n",
      "Start epoch 14/25\n",
      "Epoch 14/25, Loss: 0.2032, Accuracy: 0.93\n",
      "Start epoch 15/25\n",
      "Epoch 15/25, Loss: 0.1939, Accuracy: 0.94\n",
      "Start epoch 16/25\n",
      "Epoch 16/25, Loss: 0.1820, Accuracy: 0.94\n",
      "Start epoch 17/25\n",
      "Epoch 17/25, Loss: 0.1747, Accuracy: 0.95\n",
      "Start epoch 18/25\n",
      "Epoch 18/25, Loss: 0.1719, Accuracy: 0.95\n",
      "Start epoch 19/25\n",
      "Epoch 19/25, Loss: 0.1609, Accuracy: 0.95\n",
      "Start epoch 20/25\n",
      "Epoch 20/25, Loss: 0.1497, Accuracy: 0.95\n",
      "Start epoch 21/25\n",
      "Epoch 21/25, Loss: 0.1469, Accuracy: 0.96\n",
      "Start epoch 22/25\n",
      "Epoch 22/25, Loss: 0.1441, Accuracy: 0.96\n",
      "Start epoch 23/25\n",
      "Epoch 23/25, Loss: 0.1424, Accuracy: 0.95\n",
      "Start epoch 24/25\n",
      "Epoch 24/25, Loss: 0.1425, Accuracy: 0.96\n",
      "Start epoch 25/25\n",
      "Epoch 25/25, Loss: 0.1346, Accuracy: 0.96\n",
      "Training completed in 2411.25 seconds\n",
      "Training accuracy: 0.96\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ImageFolder(root=dataset_path_training, transform=data_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_accuracy, train_losses, computation_time = train(device, model, train_loader, criterion, optimizer, num_epochs=25)\n",
    "\n",
    "save_model(model, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.87%\n",
      "Precision: 0.92\n",
      "Recall: 0.90\n",
      "Validation Accuracy: 92.87305122494432%\n",
      "Validation Precision: 0.9181696965833633\n",
      "Validation Recall: 0.8985018213372626\n",
      "Confusion Matrix:\n",
      "[[220   2   7   0   3   3   0   0]\n",
      " [  7  89  23   2   7   1   1   1]\n",
      " [  2   3 199   3   1   1   0   1]\n",
      " [  1   1   6 108   2   2   0   0]\n",
      " [  8   5   5   2 188   1   0   1]\n",
      " [  9   2   0   1   1 609   1   1]\n",
      " [  0   1   1   0   1   5  57   1]\n",
      " [  0   1   0   0   1   0   0 198]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "validation_dataset = ImageFolder(root='../dataset-tomatoes/validation', transform=data_transforms_validation_test)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "try:\n",
    "    validation_accuracy, validation_precision, validation_recall, confusion_matrix = validate(device, model, validation_loader)\n",
    "    print(f\"Validation Accuracy: {validation_accuracy}%\")\n",
    "    print(f\"Validation Precision: {validation_precision}\")\n",
    "    print(f\"Validation Recall: {validation_recall}\")\n",
    "    print(f\"Confusion Matrix:\\n{confusion_matrix}\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"Runtime error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.87%\n",
      "Precision: 0.92\n",
      "Recall: 0.90\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_dataset = ImageFolder(root='../dataset-tomatoes/test', transform=data_transforms_validation_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_accuracy, test_precision, test_recall, test_matrix = validate(device, model, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GFLOPs: 8.1965\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchprofile\n",
    "\n",
    "load_model = torch.load(\"/Users/lorenzoperinello/Desktop/Uni/VCS/vcs-tomatoes/models-objects-saved/fine-tuning-few-layers/resnet50/resnet50-lr_0.001-batch_64-pretrained_pretrained.pth\")\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "load_model.to(device)\n",
    "\n",
    "\n",
    "# load_model.eval()\n",
    "\n",
    "# model_scripted = torch.jit.script(load_model)\n",
    "# model_scripted.save(\"mobilenetv3_large_100.pt\")\n",
    "\n",
    "input_tensor = torch.randn(1, 3, 224, 224).to(device)\n",
    "\n",
    "flops = torchprofile.profile_macs(load_model, input_tensor)  # MACs: Multiply-Accumulate operations\n",
    "\n",
    "# Since each MAC operation involves two FLOPs (one multiplication and one addition), we double the MACs\n",
    "flops *= 2\n",
    "\n",
    "# Convert FLOPs to GFLOPs (Giga FLOPs)\n",
    "gflops = flops / 1e9\n",
    "\n",
    "print(f'GFLOPs: {gflops:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
