{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import timm\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor, Normalize, Resize, Compose\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import random_split\n",
    "import time\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import MobileViTForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "class RandomOneTransform:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Randomly choose one transformation to apply\n",
    "        transform = random.choice(self.transforms)\n",
    "        return transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, filename=\"model_state_dict.pth\"):\n",
    "  torch.save(model, f\"s_{filename}.pth\")\n",
    "  torch.save(model.state_dict(), f\"s_{filename}.pth\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate(device, model, val_loader):\n",
    "    model.to(device).float()\n",
    "    model.eval()\n",
    "    num_classes = len(val_loader.dataset.classes)  # Assuming dataset classes are accessible like this\n",
    "    confusion_matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        class_correct = {}\n",
    "        class_total = {}\n",
    "        false_positives = {}\n",
    "        false_negatives = {}\n",
    "\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images).logits\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            for label, prediction in zip(labels, predicted):\n",
    "                confusion_matrix[label.item(), prediction.item()] += 1  # Update the confusion matrix\n",
    "                if label == prediction:\n",
    "                    class_correct[label.item()] = class_correct.get(label.item(), 0) + 1\n",
    "                else:\n",
    "                    false_negatives[label.item()] = false_negatives.get(label.item(), 0) + 1\n",
    "                    false_positives[prediction.item()] = false_positives.get(prediction.item(), 0) + 1\n",
    "\n",
    "                class_total[label.item()] = class_total.get(label.item(), 0) + 1\n",
    "\n",
    "        # Calculate precision and recall\n",
    "        precision_list = []\n",
    "        recall_list = []\n",
    "\n",
    "        for class_id in class_total.keys():\n",
    "            tp = class_correct.get(class_id, 0)\n",
    "            fp = false_positives.get(class_id, 0)\n",
    "            fn = false_negatives.get(class_id, 0)\n",
    "\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "            precision_list.append(precision)\n",
    "            recall_list.append(recall)\n",
    "\n",
    "        # Calculate overall precision, recall\n",
    "        overall_precision = sum(precision_list) / len(precision_list) if len(precision_list) > 0 else 0\n",
    "        overall_recall = sum(recall_list) / len(recall_list) if len(recall_list) > 0 else 0\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        print(f'Accuracy: {accuracy:.2f}%')\n",
    "        print(f'Precision: {overall_precision:.2f}')\n",
    "        print(f'Recall: {overall_recall:.2f}')\n",
    "\n",
    "        return accuracy, overall_precision, overall_recall, confusion_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device, model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.to(device).float()\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Start epoch {epoch+1}/{num_epochs}')\n",
    "        running_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracy = train_correct / train_total\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {train_accuracy:.2f}')\n",
    "\n",
    "    end = time.time()\n",
    "    computation_time = end - start \n",
    "    print(f'Training completed in {(end - start):.2f} seconds')\n",
    "    print(f'Training accuracy: {train_accuracy:.2f}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    return train_accuracy, train_losses, computation_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: mps , pretrained: pretrained\n",
      "Number of parameters in the model: 4942760\n",
      "Number of trainable parameters in the model: 108808\n",
      "mobilevit.conv_stem.convolution.weight False\n",
      "mobilevit.conv_stem.normalization.weight False\n",
      "mobilevit.conv_stem.normalization.bias False\n",
      "mobilevit.encoder.layer.0.layer.0.expand_1x1.convolution.weight False\n",
      "mobilevit.encoder.layer.0.layer.0.expand_1x1.normalization.weight False\n",
      "mobilevit.encoder.layer.0.layer.0.expand_1x1.normalization.bias False\n",
      "mobilevit.encoder.layer.0.layer.0.conv_3x3.convolution.weight False\n",
      "mobilevit.encoder.layer.0.layer.0.conv_3x3.normalization.weight False\n",
      "mobilevit.encoder.layer.0.layer.0.conv_3x3.normalization.bias False\n",
      "mobilevit.encoder.layer.0.layer.0.reduce_1x1.convolution.weight False\n",
      "mobilevit.encoder.layer.0.layer.0.reduce_1x1.normalization.weight False\n",
      "mobilevit.encoder.layer.0.layer.0.reduce_1x1.normalization.bias False\n",
      "mobilevit.encoder.layer.1.layer.0.expand_1x1.convolution.weight False\n",
      "mobilevit.encoder.layer.1.layer.0.expand_1x1.normalization.weight False\n",
      "mobilevit.encoder.layer.1.layer.0.expand_1x1.normalization.bias False\n",
      "mobilevit.encoder.layer.1.layer.0.conv_3x3.convolution.weight False\n",
      "mobilevit.encoder.layer.1.layer.0.conv_3x3.normalization.weight False\n",
      "mobilevit.encoder.layer.1.layer.0.conv_3x3.normalization.bias False\n",
      "mobilevit.encoder.layer.1.layer.0.reduce_1x1.convolution.weight False\n",
      "mobilevit.encoder.layer.1.layer.0.reduce_1x1.normalization.weight False\n",
      "mobilevit.encoder.layer.1.layer.0.reduce_1x1.normalization.bias False\n",
      "mobilevit.encoder.layer.1.layer.1.expand_1x1.convolution.weight False\n",
      "mobilevit.encoder.layer.1.layer.1.expand_1x1.normalization.weight False\n",
      "mobilevit.encoder.layer.1.layer.1.expand_1x1.normalization.bias False\n",
      "mobilevit.encoder.layer.1.layer.1.conv_3x3.convolution.weight False\n",
      "mobilevit.encoder.layer.1.layer.1.conv_3x3.normalization.weight False\n",
      "mobilevit.encoder.layer.1.layer.1.conv_3x3.normalization.bias False\n",
      "mobilevit.encoder.layer.1.layer.1.reduce_1x1.convolution.weight False\n",
      "mobilevit.encoder.layer.1.layer.1.reduce_1x1.normalization.weight False\n",
      "mobilevit.encoder.layer.1.layer.1.reduce_1x1.normalization.bias False\n",
      "mobilevit.encoder.layer.1.layer.2.expand_1x1.convolution.weight False\n",
      "mobilevit.encoder.layer.1.layer.2.expand_1x1.normalization.weight False\n",
      "mobilevit.encoder.layer.1.layer.2.expand_1x1.normalization.bias False\n",
      "mobilevit.encoder.layer.1.layer.2.conv_3x3.convolution.weight False\n",
      "mobilevit.encoder.layer.1.layer.2.conv_3x3.normalization.weight False\n",
      "mobilevit.encoder.layer.1.layer.2.conv_3x3.normalization.bias False\n",
      "mobilevit.encoder.layer.1.layer.2.reduce_1x1.convolution.weight False\n",
      "mobilevit.encoder.layer.1.layer.2.reduce_1x1.normalization.weight False\n",
      "mobilevit.encoder.layer.1.layer.2.reduce_1x1.normalization.bias False\n",
      "mobilevit.encoder.layer.2.downsampling_layer.expand_1x1.convolution.weight False\n",
      "mobilevit.encoder.layer.2.downsampling_layer.expand_1x1.normalization.weight False\n",
      "mobilevit.encoder.layer.2.downsampling_layer.expand_1x1.normalization.bias False\n",
      "mobilevit.encoder.layer.2.downsampling_layer.conv_3x3.convolution.weight False\n",
      "mobilevit.encoder.layer.2.downsampling_layer.conv_3x3.normalization.weight False\n",
      "mobilevit.encoder.layer.2.downsampling_layer.conv_3x3.normalization.bias False\n",
      "mobilevit.encoder.layer.2.downsampling_layer.reduce_1x1.convolution.weight False\n",
      "mobilevit.encoder.layer.2.downsampling_layer.reduce_1x1.normalization.weight False\n",
      "mobilevit.encoder.layer.2.downsampling_layer.reduce_1x1.normalization.bias False\n",
      "mobilevit.encoder.layer.2.conv_kxk.convolution.weight False\n",
      "mobilevit.encoder.layer.2.conv_kxk.normalization.weight False\n",
      "mobilevit.encoder.layer.2.conv_kxk.normalization.bias False\n",
      "mobilevit.encoder.layer.2.conv_1x1.convolution.weight False\n",
      "mobilevit.encoder.layer.2.transformer.layer.0.attention.attention.query.weight False\n",
      "mobilevit.encoder.layer.2.transformer.layer.0.attention.attention.query.bias False\n",
      "mobilevit.encoder.layer.2.transformer.layer.0.attention.attention.key.weight False\n",
      "mobilevit.encoder.layer.2.transformer.layer.0.attention.attention.key.bias False\n",
      "mobilevit.encoder.layer.2.transformer.layer.0.attention.attention.value.weight False\n",
      "mobilevit.encoder.layer.2.transformer.layer.0.attention.attention.value.bias False\n",
      "mobilevit.encoder.layer.2.transformer.layer.0.attention.output.dense.weight False\n",
      "mobilevit.encoder.layer.2.transformer.layer.0.attention.output.dense.bias False\n",
      "mobilevit.encoder.layer.2.transformer.layer.0.intermediate.dense.weight False\n",
      "mobilevit.encoder.layer.2.transformer.layer.0.intermediate.dense.bias False\n",
      "mobilevit.encoder.layer.2.transformer.layer.0.output.dense.weight False\n",
      "mobilevit.encoder.layer.2.transformer.layer.0.output.dense.bias False\n",
      "mobilevit.encoder.layer.2.transformer.layer.0.layernorm_before.weight False\n",
      "mobilevit.encoder.layer.2.transformer.layer.0.layernorm_before.bias False\n",
      "mobilevit.encoder.layer.2.transformer.layer.0.layernorm_after.weight False\n",
      "mobilevit.encoder.layer.2.transformer.layer.0.layernorm_after.bias False\n",
      "mobilevit.encoder.layer.2.transformer.layer.1.attention.attention.query.weight False\n",
      "mobilevit.encoder.layer.2.transformer.layer.1.attention.attention.query.bias False\n",
      "mobilevit.encoder.layer.2.transformer.layer.1.attention.attention.key.weight False\n",
      "mobilevit.encoder.layer.2.transformer.layer.1.attention.attention.key.bias False\n",
      "mobilevit.encoder.layer.2.transformer.layer.1.attention.attention.value.weight False\n",
      "mobilevit.encoder.layer.2.transformer.layer.1.attention.attention.value.bias False\n",
      "mobilevit.encoder.layer.2.transformer.layer.1.attention.output.dense.weight False\n",
      "mobilevit.encoder.layer.2.transformer.layer.1.attention.output.dense.bias False\n",
      "mobilevit.encoder.layer.2.transformer.layer.1.intermediate.dense.weight False\n",
      "mobilevit.encoder.layer.2.transformer.layer.1.intermediate.dense.bias False\n",
      "mobilevit.encoder.layer.2.transformer.layer.1.output.dense.weight False\n",
      "mobilevit.encoder.layer.2.transformer.layer.1.output.dense.bias False\n",
      "mobilevit.encoder.layer.2.transformer.layer.1.layernorm_before.weight False\n",
      "mobilevit.encoder.layer.2.transformer.layer.1.layernorm_before.bias False\n",
      "mobilevit.encoder.layer.2.transformer.layer.1.layernorm_after.weight False\n",
      "mobilevit.encoder.layer.2.transformer.layer.1.layernorm_after.bias False\n",
      "mobilevit.encoder.layer.2.layernorm.weight False\n",
      "mobilevit.encoder.layer.2.layernorm.bias False\n",
      "mobilevit.encoder.layer.2.conv_projection.convolution.weight False\n",
      "mobilevit.encoder.layer.2.conv_projection.normalization.weight False\n",
      "mobilevit.encoder.layer.2.conv_projection.normalization.bias False\n",
      "mobilevit.encoder.layer.2.fusion.convolution.weight False\n",
      "mobilevit.encoder.layer.2.fusion.normalization.weight False\n",
      "mobilevit.encoder.layer.2.fusion.normalization.bias False\n",
      "mobilevit.encoder.layer.3.downsampling_layer.expand_1x1.convolution.weight False\n",
      "mobilevit.encoder.layer.3.downsampling_layer.expand_1x1.normalization.weight False\n",
      "mobilevit.encoder.layer.3.downsampling_layer.expand_1x1.normalization.bias False\n",
      "mobilevit.encoder.layer.3.downsampling_layer.conv_3x3.convolution.weight False\n",
      "mobilevit.encoder.layer.3.downsampling_layer.conv_3x3.normalization.weight False\n",
      "mobilevit.encoder.layer.3.downsampling_layer.conv_3x3.normalization.bias False\n",
      "mobilevit.encoder.layer.3.downsampling_layer.reduce_1x1.convolution.weight False\n",
      "mobilevit.encoder.layer.3.downsampling_layer.reduce_1x1.normalization.weight False\n",
      "mobilevit.encoder.layer.3.downsampling_layer.reduce_1x1.normalization.bias False\n",
      "mobilevit.encoder.layer.3.conv_kxk.convolution.weight False\n",
      "mobilevit.encoder.layer.3.conv_kxk.normalization.weight False\n",
      "mobilevit.encoder.layer.3.conv_kxk.normalization.bias False\n",
      "mobilevit.encoder.layer.3.conv_1x1.convolution.weight False\n",
      "mobilevit.encoder.layer.3.transformer.layer.0.attention.attention.query.weight False\n",
      "mobilevit.encoder.layer.3.transformer.layer.0.attention.attention.query.bias False\n",
      "mobilevit.encoder.layer.3.transformer.layer.0.attention.attention.key.weight False\n",
      "mobilevit.encoder.layer.3.transformer.layer.0.attention.attention.key.bias False\n",
      "mobilevit.encoder.layer.3.transformer.layer.0.attention.attention.value.weight False\n",
      "mobilevit.encoder.layer.3.transformer.layer.0.attention.attention.value.bias False\n",
      "mobilevit.encoder.layer.3.transformer.layer.0.attention.output.dense.weight False\n",
      "mobilevit.encoder.layer.3.transformer.layer.0.attention.output.dense.bias False\n",
      "mobilevit.encoder.layer.3.transformer.layer.0.intermediate.dense.weight False\n",
      "mobilevit.encoder.layer.3.transformer.layer.0.intermediate.dense.bias False\n",
      "mobilevit.encoder.layer.3.transformer.layer.0.output.dense.weight False\n",
      "mobilevit.encoder.layer.3.transformer.layer.0.output.dense.bias False\n",
      "mobilevit.encoder.layer.3.transformer.layer.0.layernorm_before.weight False\n",
      "mobilevit.encoder.layer.3.transformer.layer.0.layernorm_before.bias False\n",
      "mobilevit.encoder.layer.3.transformer.layer.0.layernorm_after.weight False\n",
      "mobilevit.encoder.layer.3.transformer.layer.0.layernorm_after.bias False\n",
      "mobilevit.encoder.layer.3.transformer.layer.1.attention.attention.query.weight False\n",
      "mobilevit.encoder.layer.3.transformer.layer.1.attention.attention.query.bias False\n",
      "mobilevit.encoder.layer.3.transformer.layer.1.attention.attention.key.weight False\n",
      "mobilevit.encoder.layer.3.transformer.layer.1.attention.attention.key.bias False\n",
      "mobilevit.encoder.layer.3.transformer.layer.1.attention.attention.value.weight False\n",
      "mobilevit.encoder.layer.3.transformer.layer.1.attention.attention.value.bias False\n",
      "mobilevit.encoder.layer.3.transformer.layer.1.attention.output.dense.weight False\n",
      "mobilevit.encoder.layer.3.transformer.layer.1.attention.output.dense.bias False\n",
      "mobilevit.encoder.layer.3.transformer.layer.1.intermediate.dense.weight False\n",
      "mobilevit.encoder.layer.3.transformer.layer.1.intermediate.dense.bias False\n",
      "mobilevit.encoder.layer.3.transformer.layer.1.output.dense.weight False\n",
      "mobilevit.encoder.layer.3.transformer.layer.1.output.dense.bias False\n",
      "mobilevit.encoder.layer.3.transformer.layer.1.layernorm_before.weight False\n",
      "mobilevit.encoder.layer.3.transformer.layer.1.layernorm_before.bias False\n",
      "mobilevit.encoder.layer.3.transformer.layer.1.layernorm_after.weight False\n",
      "mobilevit.encoder.layer.3.transformer.layer.1.layernorm_after.bias False\n",
      "mobilevit.encoder.layer.3.transformer.layer.2.attention.attention.query.weight False\n",
      "mobilevit.encoder.layer.3.transformer.layer.2.attention.attention.query.bias False\n",
      "mobilevit.encoder.layer.3.transformer.layer.2.attention.attention.key.weight False\n",
      "mobilevit.encoder.layer.3.transformer.layer.2.attention.attention.key.bias False\n",
      "mobilevit.encoder.layer.3.transformer.layer.2.attention.attention.value.weight False\n",
      "mobilevit.encoder.layer.3.transformer.layer.2.attention.attention.value.bias False\n",
      "mobilevit.encoder.layer.3.transformer.layer.2.attention.output.dense.weight False\n",
      "mobilevit.encoder.layer.3.transformer.layer.2.attention.output.dense.bias False\n",
      "mobilevit.encoder.layer.3.transformer.layer.2.intermediate.dense.weight False\n",
      "mobilevit.encoder.layer.3.transformer.layer.2.intermediate.dense.bias False\n",
      "mobilevit.encoder.layer.3.transformer.layer.2.output.dense.weight False\n",
      "mobilevit.encoder.layer.3.transformer.layer.2.output.dense.bias False\n",
      "mobilevit.encoder.layer.3.transformer.layer.2.layernorm_before.weight False\n",
      "mobilevit.encoder.layer.3.transformer.layer.2.layernorm_before.bias False\n",
      "mobilevit.encoder.layer.3.transformer.layer.2.layernorm_after.weight False\n",
      "mobilevit.encoder.layer.3.transformer.layer.2.layernorm_after.bias False\n",
      "mobilevit.encoder.layer.3.transformer.layer.3.attention.attention.query.weight False\n",
      "mobilevit.encoder.layer.3.transformer.layer.3.attention.attention.query.bias False\n",
      "mobilevit.encoder.layer.3.transformer.layer.3.attention.attention.key.weight False\n",
      "mobilevit.encoder.layer.3.transformer.layer.3.attention.attention.key.bias False\n",
      "mobilevit.encoder.layer.3.transformer.layer.3.attention.attention.value.weight False\n",
      "mobilevit.encoder.layer.3.transformer.layer.3.attention.attention.value.bias False\n",
      "mobilevit.encoder.layer.3.transformer.layer.3.attention.output.dense.weight False\n",
      "mobilevit.encoder.layer.3.transformer.layer.3.attention.output.dense.bias False\n",
      "mobilevit.encoder.layer.3.transformer.layer.3.intermediate.dense.weight False\n",
      "mobilevit.encoder.layer.3.transformer.layer.3.intermediate.dense.bias False\n",
      "mobilevit.encoder.layer.3.transformer.layer.3.output.dense.weight False\n",
      "mobilevit.encoder.layer.3.transformer.layer.3.output.dense.bias False\n",
      "mobilevit.encoder.layer.3.transformer.layer.3.layernorm_before.weight False\n",
      "mobilevit.encoder.layer.3.transformer.layer.3.layernorm_before.bias False\n",
      "mobilevit.encoder.layer.3.transformer.layer.3.layernorm_after.weight False\n",
      "mobilevit.encoder.layer.3.transformer.layer.3.layernorm_after.bias False\n",
      "mobilevit.encoder.layer.3.layernorm.weight False\n",
      "mobilevit.encoder.layer.3.layernorm.bias False\n",
      "mobilevit.encoder.layer.3.conv_projection.convolution.weight False\n",
      "mobilevit.encoder.layer.3.conv_projection.normalization.weight False\n",
      "mobilevit.encoder.layer.3.conv_projection.normalization.bias False\n",
      "mobilevit.encoder.layer.3.fusion.convolution.weight False\n",
      "mobilevit.encoder.layer.3.fusion.normalization.weight False\n",
      "mobilevit.encoder.layer.3.fusion.normalization.bias False\n",
      "mobilevit.encoder.layer.4.downsampling_layer.expand_1x1.convolution.weight False\n",
      "mobilevit.encoder.layer.4.downsampling_layer.expand_1x1.normalization.weight False\n",
      "mobilevit.encoder.layer.4.downsampling_layer.expand_1x1.normalization.bias False\n",
      "mobilevit.encoder.layer.4.downsampling_layer.conv_3x3.convolution.weight False\n",
      "mobilevit.encoder.layer.4.downsampling_layer.conv_3x3.normalization.weight False\n",
      "mobilevit.encoder.layer.4.downsampling_layer.conv_3x3.normalization.bias False\n",
      "mobilevit.encoder.layer.4.downsampling_layer.reduce_1x1.convolution.weight False\n",
      "mobilevit.encoder.layer.4.downsampling_layer.reduce_1x1.normalization.weight False\n",
      "mobilevit.encoder.layer.4.downsampling_layer.reduce_1x1.normalization.bias False\n",
      "mobilevit.encoder.layer.4.conv_kxk.convolution.weight False\n",
      "mobilevit.encoder.layer.4.conv_kxk.normalization.weight False\n",
      "mobilevit.encoder.layer.4.conv_kxk.normalization.bias False\n",
      "mobilevit.encoder.layer.4.conv_1x1.convolution.weight False\n",
      "mobilevit.encoder.layer.4.transformer.layer.0.attention.attention.query.weight False\n",
      "mobilevit.encoder.layer.4.transformer.layer.0.attention.attention.query.bias False\n",
      "mobilevit.encoder.layer.4.transformer.layer.0.attention.attention.key.weight False\n",
      "mobilevit.encoder.layer.4.transformer.layer.0.attention.attention.key.bias False\n",
      "mobilevit.encoder.layer.4.transformer.layer.0.attention.attention.value.weight False\n",
      "mobilevit.encoder.layer.4.transformer.layer.0.attention.attention.value.bias False\n",
      "mobilevit.encoder.layer.4.transformer.layer.0.attention.output.dense.weight False\n",
      "mobilevit.encoder.layer.4.transformer.layer.0.attention.output.dense.bias False\n",
      "mobilevit.encoder.layer.4.transformer.layer.0.intermediate.dense.weight False\n",
      "mobilevit.encoder.layer.4.transformer.layer.0.intermediate.dense.bias False\n",
      "mobilevit.encoder.layer.4.transformer.layer.0.output.dense.weight False\n",
      "mobilevit.encoder.layer.4.transformer.layer.0.output.dense.bias False\n",
      "mobilevit.encoder.layer.4.transformer.layer.0.layernorm_before.weight False\n",
      "mobilevit.encoder.layer.4.transformer.layer.0.layernorm_before.bias False\n",
      "mobilevit.encoder.layer.4.transformer.layer.0.layernorm_after.weight False\n",
      "mobilevit.encoder.layer.4.transformer.layer.0.layernorm_after.bias False\n",
      "mobilevit.encoder.layer.4.transformer.layer.1.attention.attention.query.weight False\n",
      "mobilevit.encoder.layer.4.transformer.layer.1.attention.attention.query.bias False\n",
      "mobilevit.encoder.layer.4.transformer.layer.1.attention.attention.key.weight False\n",
      "mobilevit.encoder.layer.4.transformer.layer.1.attention.attention.key.bias False\n",
      "mobilevit.encoder.layer.4.transformer.layer.1.attention.attention.value.weight False\n",
      "mobilevit.encoder.layer.4.transformer.layer.1.attention.attention.value.bias False\n",
      "mobilevit.encoder.layer.4.transformer.layer.1.attention.output.dense.weight False\n",
      "mobilevit.encoder.layer.4.transformer.layer.1.attention.output.dense.bias False\n",
      "mobilevit.encoder.layer.4.transformer.layer.1.intermediate.dense.weight False\n",
      "mobilevit.encoder.layer.4.transformer.layer.1.intermediate.dense.bias False\n",
      "mobilevit.encoder.layer.4.transformer.layer.1.output.dense.weight False\n",
      "mobilevit.encoder.layer.4.transformer.layer.1.output.dense.bias False\n",
      "mobilevit.encoder.layer.4.transformer.layer.1.layernorm_before.weight False\n",
      "mobilevit.encoder.layer.4.transformer.layer.1.layernorm_before.bias False\n",
      "mobilevit.encoder.layer.4.transformer.layer.1.layernorm_after.weight False\n",
      "mobilevit.encoder.layer.4.transformer.layer.1.layernorm_after.bias False\n",
      "mobilevit.encoder.layer.4.transformer.layer.2.attention.attention.query.weight False\n",
      "mobilevit.encoder.layer.4.transformer.layer.2.attention.attention.query.bias False\n",
      "mobilevit.encoder.layer.4.transformer.layer.2.attention.attention.key.weight False\n",
      "mobilevit.encoder.layer.4.transformer.layer.2.attention.attention.key.bias False\n",
      "mobilevit.encoder.layer.4.transformer.layer.2.attention.attention.value.weight False\n",
      "mobilevit.encoder.layer.4.transformer.layer.2.attention.attention.value.bias False\n",
      "mobilevit.encoder.layer.4.transformer.layer.2.attention.output.dense.weight False\n",
      "mobilevit.encoder.layer.4.transformer.layer.2.attention.output.dense.bias False\n",
      "mobilevit.encoder.layer.4.transformer.layer.2.intermediate.dense.weight False\n",
      "mobilevit.encoder.layer.4.transformer.layer.2.intermediate.dense.bias False\n",
      "mobilevit.encoder.layer.4.transformer.layer.2.output.dense.weight False\n",
      "mobilevit.encoder.layer.4.transformer.layer.2.output.dense.bias False\n",
      "mobilevit.encoder.layer.4.transformer.layer.2.layernorm_before.weight False\n",
      "mobilevit.encoder.layer.4.transformer.layer.2.layernorm_before.bias False\n",
      "mobilevit.encoder.layer.4.transformer.layer.2.layernorm_after.weight False\n",
      "mobilevit.encoder.layer.4.transformer.layer.2.layernorm_after.bias False\n",
      "mobilevit.encoder.layer.4.layernorm.weight False\n",
      "mobilevit.encoder.layer.4.layernorm.bias False\n",
      "mobilevit.encoder.layer.4.conv_projection.convolution.weight False\n",
      "mobilevit.encoder.layer.4.conv_projection.normalization.weight False\n",
      "mobilevit.encoder.layer.4.conv_projection.normalization.bias False\n",
      "mobilevit.encoder.layer.4.fusion.convolution.weight False\n",
      "mobilevit.encoder.layer.4.fusion.normalization.weight False\n",
      "mobilevit.encoder.layer.4.fusion.normalization.bias False\n",
      "mobilevit.conv_1x1_exp.convolution.weight True\n",
      "mobilevit.conv_1x1_exp.normalization.weight True\n",
      "mobilevit.conv_1x1_exp.normalization.bias True\n",
      "classifier.0.weight True\n",
      "classifier.0.bias True\n"
     ]
    }
   ],
   "source": [
    "# Specify the device for training\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# Load the pre-trained MobileNetV3 model\n",
    " \n",
    "model_name = 'mobilevit-small'\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "is_pretrained = True\n",
    "is_pretrained_str = \"pretrained\" if is_pretrained else \"not_pretrained\"\n",
    "file_name = f\"{model_name}-lr_{learning_rate}-batch_{batch_size}-pretrained_{is_pretrained_str}\"\n",
    "num_classes = 8\n",
    "\n",
    "\n",
    "model = MobileViTForImageClassification.from_pretrained(\"apple/mobilevit-small\")\n",
    "    \n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_classes = 8\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(model.classifier.in_features, num_classes)\n",
    ").to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "# Define path to PlantVillage dataset\n",
    "dataset_path_training = '../dataset-tomatoes/train'\n",
    "\n",
    "# Load the PlantVillage dataset with appropriate transforms\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    RandomOneTransform([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.5),\n",
    "        transforms.ColorJitter(contrast=0.5),\n",
    "        transforms.ColorJitter(saturation=0.5),\n",
    "        transforms.RandomRotation(45),\n",
    "        transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0), ratio=(0.75, 1.33))\n",
    "    ]),\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "data_transforms_validation_test = transforms.Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# Ebable training mode\n",
    "\n",
    "# Fine-tune the model\n",
    "num_epochs = 25\n",
    "\n",
    "# freeze conv_1x1_exp layers\n",
    "for param in model.mobilevit.conv_1x1_exp.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(\"Training on:\", device, \", pretrained:\", is_pretrained_str)\n",
    "print(f\"Number of parameters in the model: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"Number of trainable parameters in the model: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch 1/25\n",
      "Epoch 1/25, Loss: 0.8226, Accuracy: 0.78\n",
      "Start epoch 2/25\n",
      "Epoch 2/25, Loss: 0.3726, Accuracy: 0.89\n",
      "Start epoch 3/25\n",
      "Epoch 3/25, Loss: 0.2904, Accuracy: 0.91\n",
      "Start epoch 4/25\n",
      "Epoch 4/25, Loss: 0.2508, Accuracy: 0.92\n",
      "Start epoch 5/25\n",
      "Epoch 5/25, Loss: 0.2262, Accuracy: 0.93\n",
      "Start epoch 6/25\n",
      "Epoch 6/25, Loss: 0.1980, Accuracy: 0.94\n",
      "Start epoch 7/25\n",
      "Epoch 7/25, Loss: 0.1918, Accuracy: 0.94\n",
      "Start epoch 8/25\n",
      "Epoch 8/25, Loss: 0.1716, Accuracy: 0.94\n",
      "Start epoch 9/25\n",
      "Epoch 9/25, Loss: 0.1642, Accuracy: 0.95\n",
      "Start epoch 10/25\n",
      "Epoch 10/25, Loss: 0.1628, Accuracy: 0.95\n",
      "Start epoch 11/25\n",
      "Epoch 11/25, Loss: 0.1483, Accuracy: 0.95\n",
      "Start epoch 12/25\n",
      "Epoch 12/25, Loss: 0.1402, Accuracy: 0.96\n",
      "Start epoch 13/25\n",
      "Epoch 13/25, Loss: 0.1357, Accuracy: 0.96\n",
      "Start epoch 14/25\n",
      "Epoch 14/25, Loss: 0.1212, Accuracy: 0.96\n",
      "Start epoch 15/25\n",
      "Epoch 15/25, Loss: 0.1136, Accuracy: 0.96\n",
      "Start epoch 16/25\n",
      "Epoch 16/25, Loss: 0.1138, Accuracy: 0.96\n",
      "Start epoch 17/25\n",
      "Epoch 17/25, Loss: 0.1036, Accuracy: 0.97\n",
      "Start epoch 18/25\n",
      "Epoch 18/25, Loss: 0.1001, Accuracy: 0.97\n",
      "Start epoch 19/25\n",
      "Epoch 19/25, Loss: 0.0972, Accuracy: 0.97\n",
      "Start epoch 20/25\n",
      "Epoch 20/25, Loss: 0.0975, Accuracy: 0.97\n",
      "Start epoch 21/25\n",
      "Epoch 21/25, Loss: 0.1040, Accuracy: 0.97\n",
      "Start epoch 22/25\n",
      "Epoch 22/25, Loss: 0.0954, Accuracy: 0.97\n",
      "Start epoch 23/25\n",
      "Epoch 23/25, Loss: 0.0932, Accuracy: 0.97\n",
      "Start epoch 24/25\n",
      "Epoch 24/25, Loss: 0.0939, Accuracy: 0.97\n",
      "Start epoch 25/25\n",
      "Epoch 25/25, Loss: 0.0816, Accuracy: 0.97\n",
      "Training completed in 2514.30 seconds\n",
      "Training accuracy: 0.97\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ImageFolder(root=dataset_path_training, transform=data_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Run training and validation\n",
    "train_accuracy, train_losses, computation_time = train(device, model, train_loader, criterion, optimizer, num_epochs=25)\n",
    "\n",
    "save_model(model, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.32%\n",
      "Precision: 0.94\n",
      "Recall: 0.93\n",
      "Validation Accuracy: 95.32293986636971%\n",
      "Validation Precision: 0.9408091604501988\n",
      "Validation Recall: 0.9289221996433807\n",
      "Confusion Matrix:\n",
      "[[224   2   1   1   5   1   0   1]\n",
      " [  3 112   8   1   4   2   0   1]\n",
      " [  0   5 203   1   0   0   1   0]\n",
      " [  1   1   3 112   0   3   0   0]\n",
      " [  5   9   2   8 184   2   0   0]\n",
      " [  0   1   0   0   0 623   0   0]\n",
      " [  0   1   3   0   1   1  57   3]\n",
      " [  0   1   0   1   0   1   0 197]]\n"
     ]
    }
   ],
   "source": [
    "validation_dataset = ImageFolder(root='../dataset-tomatoes/validation', transform=data_transforms_validation_test)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "try:\n",
    "    validation_accuracy, validation_precision, validation_recall, confusion_matrix = validate(device, model, validation_loader)\n",
    "    print(f\"Validation Accuracy: {validation_accuracy}%\")\n",
    "    print(f\"Validation Precision: {validation_precision}\")\n",
    "    print(f\"Validation Recall: {validation_recall}\")\n",
    "    print(f\"Confusion Matrix:\\n{confusion_matrix}\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"Runtime error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.32%\n",
      "Precision: 0.94\n",
      "Recall: 0.93\n"
     ]
    }
   ],
   "source": [
    "test_dataset = ImageFolder(root='../dataset-tomatoes/test', transform=data_transforms_validation_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_accuracy, test_precision, test_recall, test_matrix = validate(device, model, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          model     params  tr_params  learning_rate  batch  \\\n",
      "0          vit_tiny_patch16_224    5525960     149576          0.001     64   \n",
      "1                         vgg16  134293320   16814088          0.001     64   \n",
      "2  swin_tiny_patch4_window7_224   27525506    2366216          0.001     64   \n",
      "3            shufflenet_v2_x1_0    1261804     486080          0.001     64   \n",
      "4                      resnet50   23524424    1069064          0.001     64   \n",
      "\n",
      "   accuracy_(Tr)  accuracy_(Va)  precision_(Va)  recall_(Va)  accuracy_(Te)  \\\n",
      "0       0.921827      92.873051        0.910795     0.892480      92.873051   \n",
      "1       0.979658      95.322940        0.941256     0.925700      95.322940   \n",
      "2       0.978756      96.380846        0.954280     0.947698      96.380846   \n",
      "3       0.990489      97.438753        0.964152     0.964447      97.438753   \n",
      "4       0.958206      92.873051        0.918170     0.898502      92.873051   \n",
      "\n",
      "   precision_(Te)  recall_(Te)     time_(s)  \n",
      "0        0.910795     0.892480  1558.915665  \n",
      "1        0.941256     0.925700  4195.398219  \n",
      "2        0.954280     0.947698  4775.044521  \n",
      "3        0.964152     0.964447  1618.468904  \n",
      "4        0.918170     0.898502  2411.250841  \n"
     ]
    }
   ],
   "source": [
    "csv_path = '../results.csv'\n",
    "# append to a csv model learning_rate\tbatch\taccuracy (Tr)\tprecision (Tr)\taccuracy (Va)\tprecision (Va)\taccuracy (Te)\tprecision (Te)\ttime (s)\n",
    "# check if the file exists\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "row = pd.DataFrame({\n",
    "    'model': [model_name],\n",
    "    'params': [total_params],\n",
    "    'tr_params': [total_trainable_params],\n",
    "    'learning_rate': [learning_rate],\n",
    "    'batch': [batch_size],\n",
    "    'accuracy_(Tr)': [train_accuracy],\n",
    "    'accuracy_(Va)': [validation_accuracy],\n",
    "    'precision_(Va)': [validation_precision],\n",
    "    'recall_(Va)': [validation_recall],\n",
    "    'accuracy_(Te)': [test_accuracy],\n",
    "    'precision_(Te)': [test_precision],\n",
    "    'recall_(Te)': [test_recall],\n",
    "    'time_(s)': [computation_time]\n",
    "}, index=[0])\n",
    "print(df)\n",
    "\n",
    "# add the row to the dataframe csv file\n",
    "df = pd.concat([df, row], ignore_index=False)\n",
    "\n",
    "\n",
    "df.to_csv(csv_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
